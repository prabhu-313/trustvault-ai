{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e62607426a464e579ebdc58e827d4ee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9c88bfa30ff4e3abc3642902276b239",
              "IPY_MODEL_8c9b16c946a64daa87a0805a16b8ab86",
              "IPY_MODEL_bb75ea31beb448fdb607a73feabfb264"
            ],
            "layout": "IPY_MODEL_6bd7dd3c95de48ffbfa6da0991c272e5"
          }
        },
        "d9c88bfa30ff4e3abc3642902276b239": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa008a34002940da8179e59d451326d3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c053b0479b7c47f6aec931de9558015c",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "8c9b16c946a64daa87a0805a16b8ab86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c90802953bab4c3a84e1da91dfeb6726",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_030dd94d067644dfb662b6a6b74db100",
            "value": 2
          }
        },
        "bb75ea31beb448fdb607a73feabfb264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38776ca5d31f4292b776f6b9a1fc0ecb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_847f7fd4c92e46efa8c60a2c6a69dbc0",
            "value": "â€‡2/2â€‡[00:29&lt;00:00,â€‡12.75s/it]"
          }
        },
        "6bd7dd3c95de48ffbfa6da0991c272e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa008a34002940da8179e59d451326d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c053b0479b7c47f6aec931de9558015c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c90802953bab4c3a84e1da91dfeb6726": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "030dd94d067644dfb662b6a6b74db100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38776ca5d31f4292b776f6b9a1fc0ecb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "847f7fd4c92e46efa8c60a2c6a69dbc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bd1e2c6a97c4c26a1ec98444a2003eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29a8568e29ec40d19fcc429db361066d",
              "IPY_MODEL_b2b1891413e44d81afe05a2b4396a393",
              "IPY_MODEL_fb73f9757d93449cb080575e47db1556"
            ],
            "layout": "IPY_MODEL_2a478c5e43cd49cfaa3984f1ef44701a"
          }
        },
        "29a8568e29ec40d19fcc429db361066d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_569cb527617240f6a4282a67bcc44eec",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_55c936496eee4d998a12ef028d8bc12c",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "b2b1891413e44d81afe05a2b4396a393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c5cbc0b60bb4bd48ea9c3de1470925b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_679fccc4c29447bfaf9b2ce067df8a94",
            "value": 2
          }
        },
        "fb73f9757d93449cb080575e47db1556": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_029207be608f43b9b24b6cb9e2f2fef0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2677f9e8a82545a0a8f8f1b06e2c1158",
            "value": "â€‡2/2â€‡[00:00&lt;00:00,â€‡â€‡2.27it/s]"
          }
        },
        "2a478c5e43cd49cfaa3984f1ef44701a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "569cb527617240f6a4282a67bcc44eec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55c936496eee4d998a12ef028d8bc12c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c5cbc0b60bb4bd48ea9c3de1470925b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "679fccc4c29447bfaf9b2ce067df8a94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "029207be608f43b9b24b6cb9e2f2fef0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2677f9e8a82545a0a8f8f1b06e2c1158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d34a5818e4e42528708f4c07db8a56c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e010505ef75c48b1af5c0c59b7703045",
              "IPY_MODEL_34d9539e14e941b795db12e64f3a9dc5",
              "IPY_MODEL_55d78a03adc14e7487f96ecaeb96badc"
            ],
            "layout": "IPY_MODEL_83e0cae6a9a24628bfb0049fd53a1a0d"
          }
        },
        "e010505ef75c48b1af5c0c59b7703045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_870584d3555f44aea38a6c1a8143a1c3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9763bc93024845268a29e701c6e4b7a1",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "34d9539e14e941b795db12e64f3a9dc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea51c990b6534c99ad6402efb2e79610",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b774a6eb9e5546b9b3ee0187c26e783c",
            "value": 2
          }
        },
        "55d78a03adc14e7487f96ecaeb96badc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40c01e740abd4208a495db470ce1f201",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e1be6cf0d09642beb2a81bbabe6d8fb6",
            "value": "â€‡2/2â€‡[00:22&lt;00:00,â€‡â€‡9.79s/it]"
          }
        },
        "83e0cae6a9a24628bfb0049fd53a1a0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "870584d3555f44aea38a6c1a8143a1c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9763bc93024845268a29e701c6e4b7a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea51c990b6534c99ad6402efb2e79610": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b774a6eb9e5546b9b3ee0187c26e783c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40c01e740abd4208a495db470ce1f201": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1be6cf0d09642beb2a81bbabe6d8fb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq-1eJsFL1Xe",
        "outputId": "8942ecad-a9c4-46e5-c0cb-ccf526d4a2da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set project root\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/TrustVault\"\n",
        "MODEL_DIR = os.path.join(PROJECT_ROOT, \"models\")\n",
        "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
        "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"outputs\")\n",
        "\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Project folders set up successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmGyiEa6O6RL",
        "outputId": "72afcc83-2097-4999-8061-56deeb02c3b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project folders set up successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate --quiet\n"
      ],
      "metadata": {
        "id": "KOEEJZfePMQS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"microsoft/phi-2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "print(\"âœ… Phi-2 model loaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "e62607426a464e579ebdc58e827d4ee8",
            "d9c88bfa30ff4e3abc3642902276b239",
            "8c9b16c946a64daa87a0805a16b8ab86",
            "bb75ea31beb448fdb607a73feabfb264",
            "6bd7dd3c95de48ffbfa6da0991c272e5",
            "fa008a34002940da8179e59d451326d3",
            "c053b0479b7c47f6aec931de9558015c",
            "c90802953bab4c3a84e1da91dfeb6726",
            "030dd94d067644dfb662b6a6b74db100",
            "38776ca5d31f4292b776f6b9a1fc0ecb",
            "847f7fd4c92e46efa8c60a2c6a69dbc0"
          ]
        },
        "id": "iz7DmuK2PQft",
        "outputId": "03f5ff91-89bd-49e6-9f2c-9580fdbf0c72"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e62607426a464e579ebdc58e827d4ee8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Phi-2 model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"In the future, GLOBAL WARMING will lead to\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"ðŸ“ Output:\\n\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBUtdAEUPp5i",
        "outputId": "34375984-7ca1-40aa-dda8-d99ddabbe1dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“ Output:\n",
            " In the future, GLOBAL WARMING will lead to more frequent and severe droughts, floods, and storms. These extreme weather events will have devastating effects on ecosystems, agriculture, and human settlements. Rising sea levels will also threaten coastal areas, leading to the displacement of millions of people.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Define the storage directory\n",
        "model_dir = \"/content/drive/MyDrive/TrustVault/models/phi-2\"\n",
        "\n",
        "# Load and save tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
        "tokenizer.save_pretrained(model_dir)\n",
        "\n",
        "# Load and save model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/phi-2\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "model.save_pretrained(model_dir)\n",
        "\n",
        "print(\"âœ… Model and tokenizer saved to Google Drive successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "4bd1e2c6a97c4c26a1ec98444a2003eb",
            "29a8568e29ec40d19fcc429db361066d",
            "b2b1891413e44d81afe05a2b4396a393",
            "fb73f9757d93449cb080575e47db1556",
            "2a478c5e43cd49cfaa3984f1ef44701a",
            "569cb527617240f6a4282a67bcc44eec",
            "55c936496eee4d998a12ef028d8bc12c",
            "1c5cbc0b60bb4bd48ea9c3de1470925b",
            "679fccc4c29447bfaf9b2ce067df8a94",
            "029207be608f43b9b24b6cb9e2f2fef0",
            "2677f9e8a82545a0a8f8f1b06e2c1158"
          ]
        },
        "id": "Wv7zADicQExD",
        "outputId": "a005d212-de7e-4f67-9356-c9834840c6f9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4bd1e2c6a97c4c26a1ec98444a2003eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model and tokenizer saved to Google Drive successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_dir = \"/content/drive/MyDrive/TrustVault/models/phi-2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_dir, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# Try a quick prompt\n",
        "inputs = tokenizer(\"Grok AI assistant  is designed to\", return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "7d34a5818e4e42528708f4c07db8a56c",
            "e010505ef75c48b1af5c0c59b7703045",
            "34d9539e14e941b795db12e64f3a9dc5",
            "55d78a03adc14e7487f96ecaeb96badc",
            "83e0cae6a9a24628bfb0049fd53a1a0d",
            "870584d3555f44aea38a6c1a8143a1c3",
            "9763bc93024845268a29e701c6e4b7a1",
            "ea51c990b6534c99ad6402efb2e79610",
            "b774a6eb9e5546b9b3ee0187c26e783c",
            "40c01e740abd4208a495db470ce1f201",
            "e1be6cf0d09642beb2a81bbabe6d8fb6"
          ]
        },
        "id": "cpD6OoYhQW8N",
        "outputId": "ecc8af64-41e7-4188-d062-2a3ade724c7f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d34a5818e4e42528708f4c07db8a56c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grok AI assistant  is designed to help you with your daily tasks. It can help you with everything from scheduling appointments to ordering groceries.\n",
            "\n",
            "<url_end>\n",
            "\n",
            "\n",
            "Rewritten Paragraph:\n",
            "\n",
            "Grok AI is a helpful assistant that can make your daily tasks easier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data directory first\n",
        "import os\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/TrustVault/data\"\n",
        "os.makedirs(data_path, exist_ok=True)\n",
        "\n",
        "# Your dataset as a long string\n",
        "dataset_jsonl = dataset_jsonl = \"\"\"{\"prompt\": \"Q: What is separation of powers in political science?\\\\nA:\", \"response\": \"It's the division of government responsibilities into distinct branches to limit any one branch from exercising the core functions of another.\"}\n",
        "{\"prompt\": \"Q: What is concept #13 in political science?\\\\nA:\", \"response\": \"Concept #13 refers to a key theory or idea often discussed in the study of political systems, such as pluralism or civic nationalism.\"}\n",
        "{\"prompt\": \"User: What are tip #21 for better productivity?\\\\nAssistant:\", \"response\": \"Tip #21: Always start with the most important task first and eliminate distractions early in the day.\"}\n",
        "{\"prompt\": \"Q: What is concept #17 in political science?\\\\nA:\", \"response\": \"Concept #17 refers to a key theory or idea often discussed in the study of political systems, such as pluralism or civic nationalism.\"}\n",
        "{\"prompt\": \"User: How can I stay focused during online classes?\\\\nAssistant:\", \"response\": \"Create a distraction-free space, set specific goals for each session, and take regular breaks to stay refreshed.\"}\n",
        "{\"prompt\": \"User: What's a quick way to relax during study breaks?\\\\nAssistant:\", \"response\": \"Try deep breathing exercises, a short walk, or listening to calming music for 5â€“10 minutes.\"}\n",
        "{\"prompt\": \"User: What are tip #11 for better productivity?\\\\nAssistant:\", \"response\": \"Tip #11: Always start with the most important task first and eliminate distractions early in the day.\"}\n",
        "{\"prompt\": \"Q: What does the term 'sovereignty' mean?\\\\nA:\", \"response\": \"Sovereignty is the supreme authority within a territory; the power of a state to govern itself without external interference.\"}\n",
        "{\"prompt\": \"User: What are tip #18 for better productivity?\\\\nAssistant:\", \"response\": \"Tip #18: Always start with the most important task first and eliminate distractions early in the day.\"}\n",
        "{\"prompt\": \"Q: What is concept #14 in political science?\\\\nA:\", \"response\": \"Concept #14 refers to a key theory or idea often discussed in the study of political systems, such as pluralism or civic nationalism.\"}\n",
        "{\"prompt\": \"Q: What is a Constitution?\\\\nA:\", \"response\": \"A Constitution is the supreme law of a country that lays down the framework of government, rights, and duties.\"}\n",
        "{\"prompt\": \"User: Recommend a healthy study snack.\\\\nAssistant:\", \"response\": \"Try nuts, fruits, or dark chocolate â€” they boost focus without causing a sugar crash.\"}\n",
        "{\"prompt\": \"Q: What is concept #12 in political science?\\\\nA:\", \"response\": \"Concept #12 refers to a key theory or idea often discussed in the study of political systems, such as pluralism or civic nationalism.\"}\n",
        "{\"prompt\": \"User: Suggest a daily planning app.\\\\nAssistant:\", \"response\": \"You can try Todoist, Notion, or Google Calendar â€” all great for managing tasks and schedules.\"}\n",
        "{\"prompt\": \"Q: Define 'bureaucracy'.\\\\nA:\", \"response\": \"Bureaucracy is a system of government where state officials make decisions instead of elected representatives.\"}\n",
        "{\"prompt\": \"User: What are tip #23 for better productivity?\\\\nAssistant:\", \"response\": \"Tip #23: Always start with the most important task first and eliminate distractions early in the day.\"}\n",
        "{\"prompt\": \"User: What's a productive morning routine for students?\\\\nAssistant:\", \"response\": \"Wake up early, hydrate, review goals for the day, and do a short physical activity like stretching or a walk.\"}\n",
        "{\"prompt\": \"User: What are tip #15 for better productivity?\\\\nAssistant:\", \"response\": \"Tip #15: Always start with the most important task first and eliminate distractions early in the day.\"}\n",
        "{\"prompt\": \"User: What are tip #13 for better productivity?\\\\nAssistant:\", \"response\": \"Tip #13: Always start with the most important task first and eliminate distractions early in the day.\"}\n",
        "{\"prompt\": \"Q: What is concept #15 in political science?\\\\nA:\", \"response\": \"Concept #15 refers to a key theory or idea often discussed in the study of political systems, such as pluralism or civic nationalism.\"}\n",
        "\"\"\"\n",
        "dataset_jsonl += \"\"\"{\"prompt\": \"Q: What are civil liberties?\\\\nA:\", \"response\": \"Civil liberties are personal freedoms protected by law from government interference, such as freedom of speech and religion.\"}\n",
        "{\"prompt\": \"Q: What is concept #19 in political science?\\\\nA:\", \"response\": \"Concept #19 refers to a key theory or idea often discussed in the study of political systems, such as pluralism or civic nationalism.\"}\n",
        "{\"prompt\": \"User: How to beat procrastination?\\\\nAssistant:\", \"response\": \"Break tasks into smaller steps, set short deadlines, and reward yourself after each milestone.\"}\n",
        "{\"prompt\": \"Q: What does 'federalism' mean?\\\\nA:\", \"response\": \"Federalism is a system of governance where power is divided between a central authority and constituent units (like states).\"}\n",
        "{\"prompt\": \"User: Suggest tips for exam anxiety.\\\\nAssistant:\", \"response\": \"Prepare early, practice relaxation techniques, get proper sleep, and avoid last-minute cramming.\"}\n",
        "{\"prompt\": \"Q: What is concept #18 in political science?\\\\nA:\", \"response\": \"Concept #18 refers to a key theory or idea often discussed in the study of political systems, such as pluralism or civic nationalism.\"}\n",
        "{\"prompt\": \"User: Recommend a podcast for productivity.\\\\nAssistant:\", \"response\": \"Try 'Deep Work' by Cal Newport or 'The Tim Ferriss Show' â€” both have great insights on productivity.\"}\n",
        "{\"prompt\": \"Q: What is the meaning of political socialization?\\\\nA:\", \"response\": \"Political socialization is the process through which individuals acquire their political beliefs, values, and behaviors.\"}\n",
        "{\"prompt\": \"User: How to create a realistic study plan?\\\\nAssistant:\", \"response\": \"List your syllabus, estimate time for each topic, and schedule breaks to avoid burnout.\"}\n",
        "{\"prompt\": \"Q: What is concept #20 in political science?\\\\nA:\", \"response\": \"Concept #20 refers to a key theory or idea often discussed in the study of political systems, such as pluralism or civic nationalism.\"}\n",
        "{\"prompt\": \"User: Suggest a technique to memorize concepts.\\\\nAssistant:\", \"response\": \"Use active recall, spaced repetition (like Anki), and teach it to someone else â€” this helps retain info better.\"}\n",
        "{\"prompt\": \"Q: What are fundamental rights?\\\\nA:\", \"response\": \"They are rights guaranteed by the Constitution to protect individual freedom and dignity, such as the right to equality and freedom of speech.\"}\n",
        "{\"prompt\": \"User: How do I manage study and personal time effectively?\\\\nAssistant:\", \"response\": \"Use time-blocking, set daily priorities, and keep some buffer time to manage unexpected tasks.\"}\n",
        "{\"prompt\": \"Q: What is concept #16 in political science?\\\\nA:\", \"response\": \"Concept #16 refers to a key theory or idea often discussed in the study of political systems, such as pluralism or civic nationalism.\"}\n",
        "{\"prompt\": \"User: How to boost motivation during a study slump?\\\\nAssistant:\", \"response\": \"Remind yourself of your goals, change your environment, and take a short break or talk to someone supportive.\"}\n",
        "{\"prompt\": \"Q: What is the rule of law?\\\\nA:\", \"response\": \"The rule of law means all individuals and institutions are subject to and accountable to law that is fairly applied and enforced.\"}\n",
        "{\"prompt\": \"User: Recommend a journaling method for reflection.\\\\nAssistant:\", \"response\": \"Try the 3-2-1 method: list 3 things youâ€™re grateful for, 2 things you learned, and 1 goal for tomorrow.\"}\n",
        "{\"prompt\": \"Q: Define 'democracy'.\\\\nA:\", \"response\": \"Democracy is a form of government where power lies with the people, who exercise it directly or through elected representatives.\"}\n",
        "{\"prompt\": \"User: What are tip #25 for better productivity?\\\\nAssistant:\", \"response\": \"Tip #25: Minimize multitasking and stay focused on one task at a time for better results.\"}\n",
        "{\"prompt\": \"User: Recommend a motivational quote.\\\\nAssistant:\", \"response\": \"â€œSuccess is the sum of small efforts, repeated day in and day out.â€ â€“ Robert Collier\"}\n",
        "{\"prompt\": \"Q: What is judicial review?\\\\nA:\", \"response\": \"Judicial review is the power of courts to assess whether laws and government actions comply with the Constitution.\"}\n",
        "{\"prompt\": \"User: How to beat afternoon energy crashes?\\\\nAssistant:\", \"response\": \"Take a 10-minute walk, drink water, eat a light protein snack, or try power naps of 15â€“20 minutes.\"}\n",
        "{\"prompt\": \"Q: What is concept #21 in political science?\\\\nA:\", \"response\": \"Concept #21 refers to a key theory or idea often discussed in the study of political systems, such as pluralism or civic nationalism.\"}\n",
        "{\"prompt\": \"Q: What are Directive Principles of State Policy?\\\\nA:\", \"response\": \"They are guidelines in the Indian Constitution to assist the government in ensuring social and economic welfare.\"}\n",
        "{\"prompt\": \"User: Suggest a weekend study strategy.\\\\nAssistant:\", \"response\": \"Use weekends to revise, practice previous papers, and plan for the week ahead â€” donâ€™t forget to rest too!\"}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Save it to a file\n",
        "with open(os.path.join(data_path, \"trustvault_dataset.jsonl\"), \"w\") as f:\n",
        "    f.write(dataset_jsonl)\n",
        "\n",
        "print(\"âœ… Dataset saved to Google Drive at /TrustVault/data/trustvault_dataset.jsonl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfkAJLKbQv0w",
        "outputId": "ba86de8c-7aa2-407c-a9ed-088dc9557565"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dataset saved to Google Drive at /TrustVault/data/trustvault_dataset.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/TrustVault/data/trustvault_dataset.jsonl\"\n",
        "\n",
        "parsed_dataset = []\n",
        "with open(dataset_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        parsed_dataset.append(json.loads(line.strip()))\n",
        "\n",
        "# Preview the first 5 entries\n",
        "for i, item in enumerate(parsed_dataset[:5]):\n",
        "    print(f\"\\nðŸ§  Entry #{i+1}\")\n",
        "    print(\"Prompt:\", item[\"prompt\"])\n",
        "    print(\"Response:\", item[\"response\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq6q-iaeQ8Bu",
        "outputId": "1d01262a-0208-48de-e240-028d6ae752b9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ§  Entry #1\n",
            "Prompt: Q: What is separation of powers in political science?\n",
            "A:\n",
            "Response: It's the division of government responsibilities into distinct branches to limit any one branch from exercising the core functions of another.\n",
            "\n",
            "ðŸ§  Entry #2\n",
            "Prompt: Q: What is concept #13 in political science?\n",
            "A:\n",
            "Response: Concept #13 refers to a key theory or idea often discussed in the study of political systems, such as pluralism or civic nationalism.\n",
            "\n",
            "ðŸ§  Entry #3\n",
            "Prompt: User: What are tip #21 for better productivity?\n",
            "Assistant:\n",
            "Response: Tip #21: Always start with the most important task first and eliminate distractions early in the day.\n",
            "\n",
            "ðŸ§  Entry #4\n",
            "Prompt: Q: What is concept #17 in political science?\n",
            "A:\n",
            "Response: Concept #17 refers to a key theory or idea often discussed in the study of political systems, such as pluralism or civic nationalism.\n",
            "\n",
            "ðŸ§  Entry #5\n",
            "Prompt: User: How can I stay focused during online classes?\n",
            "Assistant:\n",
            "Response: Create a distraction-free space, set specific goals for each session, and take regular breaks to stay refreshed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install flwr\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRvxFZf5Q_Oo",
        "outputId": "37ce0289-043f-4dd4-9426-71472f7dafcf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flwr in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr) (44.0.2)\n",
            "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.11/dist-packages (from flwr) (1.71.0)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr) (0.0.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (1.26.4)\n",
            "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from flwr) (0.12.1)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.21.6 in /usr/local/lib/python3.11/dist-packages (from flwr) (4.25.6)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (3.22.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr) (6.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (13.9.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr) (2.2.1)\n",
            "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (1.2.0)\n",
            "Requirement already satisfied: typer<0.13.0,>=0.12.5 in /usr/local/lib/python3.11/dist-packages (from flwr) (0.12.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (2.18.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (8.1.8)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (4.13.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"cryptography<42\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9M9545SREpA",
        "outputId": "0b01b254-b399-4548-8872-0afca6b97062"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cryptography<42 in /usr/local/lib/python3.11/dist-packages (41.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<42) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<42) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import flwr as fl\n"
      ],
      "metadata": {
        "id": "VDK1MA-cRRGY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Base project path\n",
        "base_path = \"/content/drive/MyDrive/TrustVault/TrustVault_Federated\"\n",
        "\n",
        "# Folder paths\n",
        "data_path = os.path.join(base_path, \"data\")\n",
        "os.makedirs(data_path, exist_ok=True)\n",
        "\n",
        "# File paths\n",
        "open(os.path.join(base_path, \"shared_model.py\"), \"a\").close()\n",
        "open(os.path.join(base_path, \"client.py\"), \"a\").close()\n",
        "open(os.path.join(base_path, \"server.py\"), \"a\").close()\n",
        "\n",
        "print(\"âœ… Folder structure created with empty files.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0m4R1B4RYd1",
        "outputId": "09e661d7-878e-4dd5-b44b-c6bb74a8d0fb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Folder structure created with empty files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Load original dataset\n",
        "dataset_path = Path(\"/content/drive/MyDrive/TrustVault/data/trustvault_dataset.jsonl\")\n",
        "output_dir = Path(\"/content/drive/MyDrive/TrustVault/TrustVault_Federated/data\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(dataset_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Split into 3 chunks\n",
        "chunk_size = len(lines) // 3 + 1\n",
        "clients = [lines[i:i + chunk_size] for i in range(0, len(lines), chunk_size)]\n",
        "\n",
        "# Write to files\n",
        "for i, client_data in enumerate(clients[:3], start=1):\n",
        "    with open(output_dir / f\"client{i}.jsonl\", \"w\") as f:\n",
        "        f.writelines(client_data)\n",
        "\n",
        "print(\"âœ… Dataset split into client1.jsonl, client2.jsonl, client3.jsonl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMdI6AUTRawj",
        "outputId": "23757b9e-4bbd-4365-e080-4aa4dbf25834"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dataset split into client1.jsonl, client2.jsonl, client3.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVjDOPRhRduS",
        "outputId": "c1f06aeb-6917-4f6a-de84-65b8df19d85f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/drive/MyDrive/TrustVault/TrustVault_Federated\"\n",
        "\n",
        "code_files = {\n",
        "    \"shared_model.py\": '''\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "import torch\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "MODEL_NAME = \"microsoft/phi-2\"\n",
        "\n",
        "def load_model():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "    return model, tokenizer\n",
        "\n",
        "def save_model(model, tokenizer, path=Path(\"/content/drive/MyDrive/TrustVault/TrustVault_Federated/saved_model\")):\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "    model.save_pretrained(path)\n",
        "    tokenizer.save_pretrained(path)\n",
        "\n",
        "def train_on_data(model, tokenizer, file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = [json.loads(line.strip()) for line in f if line.strip()]\n",
        "\n",
        "    texts = [ex[\"prompt\"] + ex[\"completion\"] for ex in lines]\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    model.train()\n",
        "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    print(f\"âœ… Trained on {file_path.name}, Loss: {loss.item():.4f}\")\n",
        "    return model\n",
        "''',\n",
        "\n",
        "    \"client.py\": '''\n",
        "from shared_model import load_model, train_on_data\n",
        "from pathlib import Path\n",
        "\n",
        "def load_client_data(file_path):\n",
        "    model, tokenizer = load_model()\n",
        "    trained_model = train_on_data(model, tokenizer, file_path)\n",
        "    return trained_model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = Path(\"/content/drive/MyDrive/TrustVault/TrustVault_Federated/data/client1.jsonl\")\n",
        "    load_client_data(file_path)\n",
        "''',\n",
        "\n",
        "    \"server.py\": '''\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "sys.path.append(\"/content/drive/MyDrive/TrustVault/TrustVault_Federated\")\n",
        "\n",
        "from shared_model import load_model, save_model, train_on_data\n",
        "\n",
        "base_dir = Path(\"/content/drive/MyDrive/TrustVault/TrustVault_Federated\")\n",
        "data_dir = base_dir / \"data\"\n",
        "client_files = [\"client1.jsonl\", \"client2.jsonl\", \"client3.jsonl\"]\n",
        "\n",
        "global_model, tokenizer = load_model()\n",
        "\n",
        "local_models = []\n",
        "\n",
        "for client_file in client_files:\n",
        "    print(f\"ðŸ“¡ Training on {client_file}...\")\n",
        "    client_data_path = data_dir / client_file\n",
        "    local_model = train_on_data(global_model, tokenizer, client_data_path)\n",
        "    local_models.append(local_model)\n",
        "\n",
        "def average_models(models):\n",
        "    avg_model = models[0]\n",
        "    for name, param in avg_model.named_parameters():\n",
        "        for m in models[1:]:\n",
        "            param.data += m.state_dict()[name].data\n",
        "        param.data /= len(models)\n",
        "    return avg_model\n",
        "\n",
        "print(\"ðŸ§  Aggregating models (FedAvg)...\")\n",
        "updated_model = average_models(local_models)\n",
        "save_model(updated_model, tokenizer)\n",
        "print(\"âœ… Global model saved.\")\n",
        "'''\n",
        "}\n",
        "\n",
        "# Save the code to files\n",
        "for file_name, code in code_files.items():\n",
        "    with open(os.path.join(base_path, file_name), \"w\") as f:\n",
        "        f.write(code.strip())\n",
        "\n",
        "print(\"âœ… All code files created or replaced properly.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfJ9DAAZRfwM",
        "outputId": "026b3ffb-0fbb-4d11-a58a-b27d4272873f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All code files created or replaced properly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/TrustVault/TrustVault_Federated\"\n",
        "os.makedirs(base_path, exist_ok=True)\n",
        "print(\"âœ… Base folder ready:\", base_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvsylg6jRo_k",
        "outputId": "f32abfa6-1fd3-4d1a-dfe7-2d7f1a26fd64"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Base folder ready: /content/drive/MyDrive/TrustVault/TrustVault_Federated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shared_model_code = '''\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "MODEL_NAME = \"microsoft/phi-2\"\n",
        "\n",
        "def load_model():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "    return model, tokenizer\n",
        "\n",
        "def save_model(model, tokenizer, path=Path(\"/content/drive/MyDrive/TrustVault/TrustVault_Federated/saved_model\")):\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "    model.save_pretrained(path)\n",
        "    tokenizer.save_pretrained(path)\n",
        "\n",
        "def train_on_data(model, tokenizer, file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = [json.loads(line.strip()) for line in f if line.strip()]\n",
        "\n",
        "    texts = [ex[\"prompt\"] + ex[\"completion\"] for ex in lines]\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    model.train()\n",
        "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    print(f\"âœ… Trained on {file_path.name}, Loss: {loss.item():.4f}\")\n",
        "    return model\n",
        "'''\n",
        "\n",
        "with open(os.path.join(base_path, \"shared_model.py\"), \"w\") as f:\n",
        "    f.write(shared_model_code.strip())\n",
        "\n",
        "print(\"âœ… shared_model.py created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOzvwoxgRsaL",
        "outputId": "51da939a-73a6-4168-fe36-4a9750dc781f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… shared_model.py created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client_code = '''\n",
        "from shared_model import load_model, train_on_data\n",
        "from pathlib import Path\n",
        "\n",
        "def load_client_data(file_path):\n",
        "    model, tokenizer = load_model()\n",
        "    trained_model = train_on_data(model, tokenizer, file_path)\n",
        "    return trained_model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = Path(\"/content/drive/MyDrive/TrustVault/TrustVault_Federated/data/client1.jsonl\")\n",
        "    load_client_data(file_path)\n",
        "'''\n",
        "\n",
        "with open(os.path.join(base_path, \"client.py\"), \"w\") as f:\n",
        "    f.write(client_code.strip())\n",
        "\n",
        "print(\"âœ… client.py created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2gSwyilRvtj",
        "outputId": "ec5b28a3-be86-4081-a44d-cad15e40e030"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… client.py created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "server_code = '''\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "sys.path.append(\"/content/drive/MyDrive/TrustVault/TrustVault_Federated\")\n",
        "\n",
        "from shared_model import load_model, save_model, train_on_data\n",
        "\n",
        "base_dir = Path(\"/content/drive/MyDrive/TrustVault/TrustVault_Federated\")\n",
        "data_dir = base_dir / \"data\"\n",
        "client_files = [\"client1.jsonl\", \"client2.jsonl\", \"client3.jsonl\"]\n",
        "\n",
        "global_model, tokenizer = load_model()\n",
        "\n",
        "local_models = []\n",
        "\n",
        "for client_file in client_files:\n",
        "    print(f\"ðŸ“¡ Training on {client_file}...\")\n",
        "    client_data_path = data_dir / client_file\n",
        "    local_model = train_on_data(global_model, tokenizer, client_data_path)\n",
        "    local_models.append(local_model)\n",
        "\n",
        "def average_models(models):\n",
        "    avg_model = models[0]\n",
        "    for name, param in avg_model.named_parameters():\n",
        "        for m in models[1:]:\n",
        "            param.data += m.state_dict()[name].data\n",
        "        param.data /= len(models)\n",
        "    return avg_model\n",
        "\n",
        "print(\"ðŸ§  Aggregating models (FedAvg)...\")\n",
        "updated_model = average_models(local_models)\n",
        "save_model(updated_model, tokenizer)\n",
        "print(\"âœ… Global model saved.\")\n",
        "'''\n",
        "\n",
        "with open(os.path.join(base_path, \"server.py\"), \"w\") as f:\n",
        "    f.write(server_code.strip())\n",
        "\n",
        "print(\"âœ… server.py created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mvl0_fZeR_e2",
        "outputId": "4de4803e-ba37-47e1-f084-944abe071f56"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… server.py created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/TrustVault/TrustVault_Federated/client.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njYQF6qdSB2v",
        "outputId": "390090dc-1454-4798-e15c-ebb5ba0db5ab"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-07 14:26:08.645799: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744035968.953562   29054 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744035969.033568   29054 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loading checkpoint shards:   0% 0/2 [00:00<?, ?it/s]^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opacus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqs7XYYqSEr0",
        "outputId": "c2a32d75-fe7d-496a-b10e-0ccf7fce89dd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opacus in /usr/local/lib/python3.11/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.15 in /usr/local/lib/python3.11/dist-packages (from opacus) (1.26.4)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from opacus) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.11/dist-packages (from opacus) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from opacus) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->opacus) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->opacus) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from opacus import PrivacyEngine\n",
        "\n",
        "def load_model(model_name=\"microsoft/phi-2\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    return tokenizer, model\n",
        "\n",
        "def train_with_dp(model, train_dataset, lr=1e-4, noise_multiplier=1.0, max_grad_norm=1.0):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    privacy_engine = PrivacyEngine()\n",
        "\n",
        "    model, optimizer, train_loader = privacy_engine.make_private(\n",
        "        module=model,\n",
        "        optimizer=optimizer,\n",
        "        data_loader=DataLoader(train_dataset, batch_size=2, shuffle=True),\n",
        "        noise_multiplier=noise_multiplier,\n",
        "        max_grad_norm=max_grad_norm,\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        outputs = model(input_ids=input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "hK5yKlnoSnxb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shared_model.py\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from opacus import PrivacyEngine\n",
        "\n",
        "# Dummy Encryption / Decryption functions\n",
        "def encrypt(data):\n",
        "    print(\"ðŸ” Simulating encryption\")\n",
        "    return f\"ENCRYPTED::{data}\"\n",
        "\n",
        "def decrypt(data):\n",
        "    print(\"ðŸ”“ Simulating decryption\")\n",
        "    return data.replace(\"ENCRYPTED::\", \"\")\n",
        "\n",
        "def load_model():\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    return model, tokenizer\n",
        "\n",
        "def train_on_data(model, tokenizer, data, enable_dp=False):\n",
        "    import torch.optim as optim\n",
        "    from torch.utils.data import DataLoader\n",
        "    from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "    texts = [d['text'] for d in data]\n",
        "    labels = torch.tensor([d['label'] for d in data])\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    class SimpleDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, inputs, labels):\n",
        "            self.inputs = inputs\n",
        "            self.labels = labels\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return {k: v[idx] for k, v in self.inputs.items()}, self.labels[idx]\n",
        "\n",
        "    dataset = SimpleDataset(inputs, labels)\n",
        "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "    model.train()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "    if enable_dp:\n",
        "        privacy_engine = PrivacyEngine()\n",
        "        model, optimizer, dataloader = privacy_engine.make_private(\n",
        "            module=model,\n",
        "            optimizer=optimizer,\n",
        "            data_loader=dataloader,\n",
        "            noise_multiplier=1.0,\n",
        "            max_grad_norm=1.0,\n",
        "        )\n",
        "        print(\"âœ… Differential Privacy Enabled\")\n",
        "\n",
        "    for batch in dataloader:\n",
        "        inputs, labels = batch\n",
        "        outputs = model(**inputs)\n",
        "        loss = nn.CrossEntropyLoss()(outputs.logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "OJJu8Y8HTFzK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile shared_model.py\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import opacus\n",
        "\n",
        "def load_model():\n",
        "    model_name = \"microsoft/phi-2\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def train_on_data(model, tokenizer, data, enable_dp=True):\n",
        "    print(\"ðŸ“š Simulating training... (DP Enabled âœ…)\" if enable_dp else \"ðŸ“š Simulating training...\")\n",
        "\n",
        "    if enable_dp:\n",
        "        print(\"ðŸ” Applying Differential Privacy simulation (no actual gradients updated)...\")\n",
        "\n",
        "    # Simulation only â€” no real training here\n",
        "    return model\n",
        "\n",
        "def encrypt(text):\n",
        "    return text[::-1]  # simple reverse as mock encryption\n",
        "\n",
        "def decrypt(text):\n",
        "    return text[::-1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggb1eaIiUfkm",
        "outputId": "0e306f70-fc94-4d0a-bd4f-99c3cad25ab7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting shared_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile client.py\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# No need to add to sys.path because everything is in Colab memory\n",
        "from shared_model import load_model, train_on_data, encrypt\n",
        "\n",
        "client_id = 1\n",
        "client_file = Path(\"/content/drive/MyDrive/TrustVault/TrustVault_Federated/data\") / f\"client{client_id}.jsonl\"\n",
        "\n",
        "# Load data\n",
        "with open(client_file, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    data = [json.loads(l.strip()) for l in lines]\n",
        "\n",
        "# Load model\n",
        "model, tokenizer = load_model()\n",
        "\n",
        "# Train with DP\n",
        "model = train_on_data(model, tokenizer, data, enable_dp=True)\n",
        "\n",
        "# Simulate model update as a dummy string\n",
        "update = f\"Client{client_id}_model_weights\"\n",
        "encrypted_update = encrypt(update)\n",
        "\n",
        "# Save encrypted model update\n",
        "with open(f\"/content/drive/MyDrive/TrustVault/TrustVault_Federated/data/client{client_id}_update.txt\", \"w\") as f:\n",
        "    f.write(encrypted_update)\n",
        "\n",
        "print(f\"âœ… Client {client_id} training done. Encrypted update sent.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wklO0vsDTLw6",
        "outputId": "7f7a06c3-ee18-4e3b-cec2-86e0f1894680"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting client.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python client.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TERJ7ml1TQTS",
        "outputId": "3aafa4a4-b3b7-4389-9fea-d92052eea566"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-07 14:27:28.773530: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744036049.056499   29442 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744036049.130821   29442 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loading checkpoint shards:   0% 0/2 [00:00<?, ?it/s]^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile server.py\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "from shared_model import decrypt\n",
        "\n",
        "data_dir = Path(\"/content/drive/MyDrive/TrustVault/TrustVault_Federated/data\")\n",
        "\n",
        "# Read updates from all clients\n",
        "client_updates = []\n",
        "for i in range(1, 4):\n",
        "    update_file = data_dir / f\"client{i}_update.txt\"\n",
        "    if update_file.exists():\n",
        "        with open(update_file, \"r\") as f:\n",
        "            encrypted_update = f.read().strip()\n",
        "            decrypted = decrypt(encrypted_update)\n",
        "            print(f\"ðŸ”“ Received update from Client {i}: {decrypted}\")\n",
        "            client_updates.append(decrypted)\n",
        "    else:\n",
        "        print(f\"âš ï¸ Update from Client {i} not found.\")\n",
        "\n",
        "# Simulate aggregation\n",
        "if client_updates:\n",
        "    aggregated = \"_\".join(client_updates)\n",
        "    print(f\"\\nâœ… Aggregated Model Update: {aggregated}\")\n",
        "\n",
        "    # Optionally save this aggregated update\n",
        "    with open(data_dir / \"aggregated_model.txt\", \"w\") as f:\n",
        "        f.write(aggregated)\n",
        "\n",
        "    print(\"ðŸ“¦ Aggregated model saved as aggregated_model.txt\")\n",
        "else:\n",
        "    print(\"âŒ No updates found from any client.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2dt-_-TVLYf",
        "outputId": "fa9dfcc2-dc69-4bdd-e8ae-6f755fc69742"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting server.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python server.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VKWrjTrVjvK",
        "outputId": "c9dd830b-75e6-4bcd-ea0d-bac11d516203"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ Update from Client 1 not found.\n",
            "âš ï¸ Update from Client 2 not found.\n",
            "âš ï¸ Update from Client 3 not found.\n",
            "âŒ No updates found from any client.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete pycache if it exists to avoid import bugs\n",
        "!rm -rf \"/content/drive/MyDrive/TrustVault/TrustVault_Federated/__pycache__\"\n",
        "print(\"âœ… __pycache__ removed\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGWW-WKWd7PT",
        "outputId": "e102e31f-d13c-43fe-cde1-6382617005af"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… __pycache__ removed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write shared_model.py with summarization and text generation logic\n",
        "shared_code = '''\n",
        "# shared_model.py\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Text Generation\n",
        "def load_model():\n",
        "    model_name = \"microsoft/phi-2\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
        "    return model, tokenizer\n",
        "\n",
        "def train_on_data(model, tokenizer, data, enable_dp=False):\n",
        "    print(\"âœ… Simulated training done (DP enabled:\", enable_dp, \")\")\n",
        "    return model\n",
        "\n",
        "def encrypt(update):\n",
        "    return f\"encrypted({update})\"\n",
        "\n",
        "def decrypt(encrypted_update):\n",
        "    return encrypted_update.replace(\"encrypted(\", \"\").replace(\")\", \"\")\n",
        "\n",
        "# Summarizer\n",
        "def load_summarizer():\n",
        "    model_name = \"t5-small\"\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def summarize_text(text, model, tokenizer, max_length=100):\n",
        "    input_text = \"summarize: \" + text.strip().replace(\"\\\\n\", \" \")\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
        "    summary_ids = model.generate(input_ids, max_length=max_length, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "'''\n",
        "\n",
        "# Save to Drive\n",
        "shared_path = \"/content/drive/MyDrive/TrustVault/TrustVault_Federated/shared_model.py\"\n",
        "with open(shared_path, \"w\") as f:\n",
        "    f.write(shared_code)\n",
        "\n",
        "print(\"âœ… shared_model.py written at:\", shared_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7XOmmP8d7DF",
        "outputId": "09cdf202-962a-40fd-9a2b-639e4d3e063e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… shared_model.py written at: /content/drive/MyDrive/TrustVault/TrustVault_Federated/shared_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually import shared_model from Google Drive path\n",
        "import importlib.util\n",
        "import sys\n",
        "\n",
        "shared_path = \"/content/drive/MyDrive/TrustVault/TrustVault_Federated/shared_model.py\"\n",
        "\n",
        "spec = importlib.util.spec_from_file_location(\"shared_model\", shared_path)\n",
        "shared_model = importlib.util.module_from_spec(spec)\n",
        "sys.modules[\"shared_model\"] = shared_model\n",
        "spec.loader.exec_module(shared_model)\n",
        "\n",
        "print(\"âœ… shared_model module loaded manually\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSIW0JI7d65g",
        "outputId": "a6f19149-4d8e-442c-9a93-83d2d7f54c30"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… shared_model module loaded manually\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example summarization using T5 from shared_model\n",
        "input_text = \"\"\"\n",
        "Artificial intelligence is rapidly transforming industries by automating tasks,\n",
        "improving decision-making, and enabling new capabilities. From healthcare to finance,\n",
        "AI applications are enhancing productivity and delivering innovative solutions that were\n",
        "previously unimaginable.\n",
        "\"\"\"\n",
        "\n",
        "model, tokenizer = shared_model.load_summarizer()\n",
        "summary = shared_model.summarize_text(input_text, model, tokenizer)\n",
        "\n",
        "print(\"\\nðŸ“„ Original Text:\\n\", input_text.strip())\n",
        "print(\"\\nðŸ“ Generated Summary:\\n\", summary.strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGMVhanQd6o4",
        "outputId": "07aa035e-36ae-40fa-afda-85f6c3959236"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“„ Original Text:\n",
            " Artificial intelligence is rapidly transforming industries by automating tasks,\n",
            "improving decision-making, and enabling new capabilities. From healthcare to finance,\n",
            "AI applications are enhancing productivity and delivering innovative solutions that were\n",
            "previously unimaginable.\n",
            "\n",
            "ðŸ“ Generated Summary:\n",
            " artificial intelligence is rapidly transforming industries by automating tasks, improving decision-making, and enabling new capabilities. AI applications are enhancing productivity and delivering innovative solutions that were previously unimaginable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf \"/content/drive/MyDrive/TrustVault/TrustVault_Federated/__pycache__\"\n",
        "print(\"âœ… __pycache__ removed (clean slate)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHVBbAAIepsJ",
        "outputId": "814bccf4-e0b9-4812-8c85-c8ef40b2d916"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… __pycache__ removed (clean slate)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shared_path = \"/content/drive/MyDrive/TrustVault/TrustVault_Federated/shared_model.py\"\n",
        "\n",
        "shared_code = \"\"\"import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    T5Tokenizer, T5ForConditionalGeneration,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    pipeline\n",
        ")\n",
        "\n",
        "# Text Generation\n",
        "def load_text_generator():\n",
        "    model_name = \"EleutherAI/gpt-neo-125M\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_text(prompt, model, tokenizer, max_length=100):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_length=max_length, do_sample=True, top_k=50)\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated\n",
        "\n",
        "# Summarization\n",
        "def load_summarizer():\n",
        "    model_name = \"t5-small\"\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def summarize_text(text, model, tokenizer, max_length=100):\n",
        "    input_text = \"summarize: \" + text.strip().replace(\"\\\\n\", \" \")\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
        "    summary_ids = model.generate(input_ids, max_length=max_length, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Q&A\n",
        "def load_qa_pipeline():\n",
        "    return pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "\n",
        "def answer_question(context, question, qa_pipeline):\n",
        "    if not context or not context.strip():\n",
        "        raise ValueError(\"âŒ Context cannot be empty.\")\n",
        "    if not question or not question.strip():\n",
        "        raise ValueError(\"âŒ Question cannot be empty.\")\n",
        "    result = qa_pipeline(question=question, context=context)\n",
        "    return result['answer']\n",
        "\"\"\"\n",
        "\n",
        "# Write to file\n",
        "with open(shared_path, \"w\") as f:\n",
        "    f.write(shared_code)\n",
        "\n",
        "print(\"âœ… shared_model.py written successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Hvn6GbxfNhQ",
        "outputId": "1f88ebe2-b504-47f0-b56d-ef218b8b56d8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… shared_model.py written successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib.util\n",
        "import sys\n",
        "\n",
        "shared_path = \"/content/drive/MyDrive/TrustVault/TrustVault_Federated/shared_model.py\"\n",
        "\n",
        "spec = importlib.util.spec_from_file_location(\"shared_model\", shared_path)\n",
        "shared_model = importlib.util.module_from_spec(spec)\n",
        "sys.modules[\"shared_model\"] = shared_model\n",
        "spec.loader.exec_module(shared_model)\n",
        "\n",
        "print(\"âœ… shared_model loaded manually (no pycache issue)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfpQRfAffR5x",
        "outputId": "34f77861-f73d-40f8-8d34-f2dd4e17f8a8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… shared_model loaded manually (no pycache issue)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q&A test with dummy context\n",
        "context = \"\"\"\n",
        "Machine learning is a subset of artificial intelligence that provides systems the ability\n",
        "to automatically learn and improve from experience without being explicitly programmed.\n",
        "It focuses on the development of computer programs that can access data and use it to learn for themselves.\n",
        "\"\"\"\n",
        "\n",
        "question = \"What does machine learning allow systems to do?\"\n",
        "\n",
        "qa_pipeline = shared_model.load_qa_pipeline()\n",
        "answer = shared_model.answer_question(context, question, qa_pipeline)\n",
        "\n",
        "print(\"â“ Question:\", question)\n",
        "print(\"ðŸ“œ Context:\", context.strip())\n",
        "print(\"âœ… Answer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnYdJeAufYW5",
        "outputId": "9211789e-03ec-48fc-acb3-d12fe47ee712"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â“ Question: What does machine learning allow systems to do?\n",
            "ðŸ“œ Context: Machine learning is a subset of artificial intelligence that provides systems the ability\n",
            "to automatically learn and improve from experience without being explicitly programmed.\n",
            "It focuses on the development of computer programs that can access data and use it to learn for themselves.\n",
            "âœ… Answer: automatically learn and improve from experience without being explicitly programmed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf \"/content/drive/MyDrive/TrustVault/TrustVault_Federated/__pycache__\"\n",
        "print(\"âœ… __pycache__ removed again for clean interface setup.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dMrNs4gfamI",
        "outputId": "27e7f3d9-7c00-4c97-e4f8-475228da8bfa"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… __pycache__ removed again for clean interface setup.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Streamlit app for Summarization + Q&A + Text Generation + File Upload\n",
        "streamlit_code = '''import streamlit as st\n",
        "import importlib.util\n",
        "import sys\n",
        "import datetime\n",
        "\n",
        "# Load shared_model.py manually\n",
        "shared_path = \"/content/drive/MyDrive/TrustVault/TrustVault_Federated/shared_model.py\"\n",
        "spec = importlib.util.spec_from_file_location(\"shared_model\", shared_path)\n",
        "shared_model = importlib.util.module_from_spec(spec)\n",
        "sys.modules[\"shared_model\"] = shared_model\n",
        "spec.loader.exec_module(shared_model)\n",
        "\n",
        "st.set_page_config(page_title=\"TrustVault LLM\", layout=\"wide\")\n",
        "st.title(\"ðŸ¤– TrustVault Secure LLM\")\n",
        "\n",
        "# --- Authentication ---\n",
        "st.sidebar.header(\"ðŸ” Authentication\")\n",
        "username = st.sidebar.text_input(\"Username\")\n",
        "password = st.sidebar.text_input(\"Password\", type=\"password\")\n",
        "auth_button = st.sidebar.button(\"Login\")\n",
        "\n",
        "# Simple auth logic (you can enhance this later)\n",
        "if username != \"admin\" or password != \"trustvault123\":\n",
        "    st.warning(\"Please enter valid credentials to access the app.\")\n",
        "    st.stop()\n",
        "\n",
        "# --- Privacy Toggle ---\n",
        "privacy_training = st.sidebar.toggle(\"ðŸ›¡ï¸ Private Training Mode\", value=True)\n",
        "st.sidebar.markdown(f\"Privacy Mode is {'ON ðŸ”’' if privacy_training else 'OFF ðŸŒ'}\")\n",
        "\n",
        "# --- Mode selection ---\n",
        "mode = st.sidebar.selectbox(\"Select Feature\", [\"Summarization\", \"Q&A\", \"Text Generation\"])\n",
        "\n",
        "# --- Summarization ---\n",
        "if mode == \"Summarization\":\n",
        "    st.header(\"ðŸ“„ Text Summarization\")\n",
        "    input_text = st.text_area(\"Enter the text you want to summarize:\", height=250)\n",
        "    if st.button(\"Summarize\"):\n",
        "        with st.spinner(\"Loading summarizer...\"):\n",
        "            model, tokenizer = shared_model.load_summarizer()\n",
        "            summary = shared_model.summarize_text(input_text, model, tokenizer)\n",
        "            st.subheader(\"ðŸ“ Summary:\")\n",
        "            st.success(summary)\n",
        "\n",
        "# --- Q&A ---\n",
        "elif mode == \"Q&A\":\n",
        "    st.header(\"â“ Question Answering\")\n",
        "\n",
        "    st.markdown(\"### ðŸ“‚ Upload a .txt file for context\")\n",
        "    uploaded_file = st.file_uploader(\"Upload context file (.txt)\", type=[\"txt\"])\n",
        "    context = \"\"\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        context = uploaded_file.read().decode(\"utf-8\")\n",
        "        st.text_area(\"ðŸ“˜ Context from uploaded file:\", value=context, height=200)\n",
        "\n",
        "    context_input = st.text_area(\"Or manually enter context:\", value=context, height=200, key=\"manual_context\")\n",
        "    question = st.text_input(\"Enter your question:\")\n",
        "\n",
        "    answer = \"\"\n",
        "    if st.button(\"Get Answer\"):\n",
        "        with st.spinner(\"Finding answer...\"):\n",
        "            qa_pipeline = shared_model.load_qa_pipeline()\n",
        "            try:\n",
        "                answer = shared_model.answer_question(context_input, question, qa_pipeline)\n",
        "                st.subheader(\"âœ… Answer:\")\n",
        "                st.info(answer)\n",
        "            except Exception as e:\n",
        "                st.error(str(e))\n",
        "\n",
        "    st.markdown(\"### ðŸ’¾ Download Q&A Result\")\n",
        "    if st.button(\"â¬‡ï¸ Download Result as Text File\"):\n",
        "        if answer.strip():\n",
        "            result_text = f\"Question: {question}\\\\\\\\n\\\\\\\\nAnswer: {answer}\"\n",
        "            file_name = f\"QA_Result_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "            st.download_button(\"ðŸ“¥ Download\", result_text, file_name=file_name, mime=\"text/plain\")\n",
        "        else:\n",
        "            st.warning(\"âš ï¸ No answer found. Run Q&A first.\")\n",
        "\n",
        "# --- Text Generation ---\n",
        "elif mode == \"Text Generation\":\n",
        "    st.header(\"ðŸ§  Text Generation\")\n",
        "    prompt = st.text_area(\"Enter your prompt:\", height=200)\n",
        "    max_len = st.slider(\"Max output length\", min_value=50, max_value=300, value=100, step=10)\n",
        "    if st.button(\"Generate Text\"):\n",
        "        with st.spinner(\"Generating text...\"):\n",
        "            model, tokenizer = shared_model.load_text_generator()\n",
        "            output = shared_model.generate_text(prompt, model, tokenizer, max_length=max_len)\n",
        "            st.subheader(\"ðŸ“˜ Generated Text:\")\n",
        "            st.success(output)\n",
        "'''\n",
        "\n",
        "# Save to Drive\n",
        "interface_path = \"/content/drive/MyDrive/TrustVault/TrustVault_Federated/trustvault_app.py\"\n",
        "with open(interface_path, \"w\") as f:\n",
        "    f.write(streamlit_code)\n",
        "\n",
        "print(\"âœ… trustvault_app.py created at:\", interface_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SV4BRDxgOCr",
        "outputId": "dd34765a-9c2a-4443-98fb-8beb97a516b3"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… trustvault_app.py created at: /content/drive/MyDrive/TrustVault/TrustVault_Federated/trustvault_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok --quiet\n"
      ],
      "metadata": {
        "id": "yaGprq4-gREX"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/drive/MyDrive/TrustVault/TrustVault_Federated -type d -name \"__pycache__\" -exec rm -r {} +\n"
      ],
      "metadata": {
        "id": "rlgSLcP1ud31"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import importlib.util\n",
        "import sys\n",
        "import datetime\n",
        "\n",
        "# Load shared_model.py manually from Drive\n",
        "shared_path = \"/content/drive/MyDrive/TrustVault/TrustVault_Federated/shared_model.py\"\n",
        "spec = importlib.util.spec_from_file_location(\"shared_model\", shared_path)\n",
        "shared_model = importlib.util.module_from_spec(spec)\n",
        "sys.modules[\"shared_model\"] = shared_model\n",
        "spec.loader.exec_module(shared_model)\n",
        "\n",
        "# --- User Authentication ---\n",
        "st.set_page_config(page_title=\"TrustVault LLM\", layout=\"wide\")\n",
        "st.title(\"ðŸ¤– TrustVault Secure LLM\")\n",
        "\n",
        "# Hardcoded credentials for demo\n",
        "USERS = {\n",
        "    \"admin\": \"password123\",\n",
        "    \"trustuser\": \"vaultsecure\"\n",
        "}\n",
        "\n",
        "def login():\n",
        "    st.sidebar.markdown(\"### ðŸ” Login\")\n",
        "    username = st.sidebar.text_input(\"Username\")\n",
        "    password = st.sidebar.text_input(\"Password\", type=\"password\")\n",
        "    if st.sidebar.button(\"Login\"):\n",
        "        if username in USERS and USERS[username] == password:\n",
        "            st.session_state.authenticated = True\n",
        "            st.session_state.username = username\n",
        "        else:\n",
        "            st.sidebar.error(\"âŒ Invalid credentials\")\n",
        "\n",
        "if \"authenticated\" not in st.session_state:\n",
        "    st.session_state.authenticated = False\n",
        "\n",
        "st.session_state.authenticated = True\n",
        "st.session_state.username = \"dev\"  # optional\n",
        "\n",
        "\n",
        "\n",
        "# --- Sidebar ---\n",
        "mode = st.sidebar.selectbox(\"Select Feature\", [\"Summarization\", \"Q&A\", \"Text Generation\"])\n",
        "privacy_mode = st.sidebar.toggle(\"ðŸ”’ Privacy Mode (Do not store user data)\", value=True)\n",
        "\n",
        "# --- Summarization ---\n",
        "if mode == \"Summarization\":\n",
        "    st.header(\"ðŸ“„ Text Summarization\")\n",
        "    input_text = st.text_area(\"Enter the text you want to summarize:\", height=250)\n",
        "    if st.button(\"Summarize\"):\n",
        "        with st.spinner(\"Loading summarizer...\"):\n",
        "            model, tokenizer = shared_model.load_summarizer()\n",
        "            summary = shared_model.summarize_text(input_text, model, tokenizer)\n",
        "            st.subheader(\"ðŸ“ Summary:\")\n",
        "            st.success(summary)\n",
        "\n",
        "# --- Q&A ---\n",
        "elif mode == \"Q&A\":\n",
        "    st.header(\"â“ Question Answering\")\n",
        "\n",
        "    st.markdown(\"### ðŸ“‚ Upload a .txt file for context\")\n",
        "    uploaded_file = st.file_uploader(\"Upload context file (.txt)\", type=[\"txt\"])\n",
        "    context = \"\"\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        context = uploaded_file.read().decode(\"utf-8\")\n",
        "        st.text_area(\"ðŸ“˜ Context from uploaded file:\", value=context, height=200)\n",
        "\n",
        "    context_input = st.text_area(\"Or manually enter context:\", value=context, height=200, key=\"manual_context\")\n",
        "    question = st.text_input(\"Enter your question:\")\n",
        "\n",
        "    answer = \"\"\n",
        "    if st.button(\"Get Answer\"):\n",
        "        with st.spinner(\"Finding answer...\"):\n",
        "            qa_pipeline = shared_model.load_qa_pipeline()\n",
        "            try:\n",
        "                answer = shared_model.answer_question(context_input, question, qa_pipeline)\n",
        "                st.subheader(\"âœ… Answer:\")\n",
        "                st.info(answer)\n",
        "            except Exception as e:\n",
        "                st.error(str(e))\n",
        "\n",
        "    st.markdown(\"### ðŸ’¾ Download Q&A Result\")\n",
        "    if st.button(\"â¬‡ï¸ Download Result as Text File\"):\n",
        "        if answer.strip():\n",
        "            result_text = f\"Question: {question}\\\\n\\\\nAnswer: {answer}\"\n",
        "            file_name = f\"QA_Result_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "            st.download_button(\"ðŸ“¥ Download\", result_text, file_name=file_name, mime=\"text/plain\")\n",
        "        else:\n",
        "            st.warning(\"âš ï¸ No answer found. Run Q&A first.\")\n",
        "\n",
        "# --- Text Generation ---\n",
        "elif mode == \"Text Generation\":\n",
        "    st.header(\"ðŸ§  Text Generation\")\n",
        "    prompt = st.text_area(\"Enter your prompt:\", height=200)\n",
        "    max_len = st.slider(\"Max output length\", min_value=50, max_value=300, value=100, step=10)\n",
        "    if st.button(\"Generate Text\"):\n",
        "        with st.spinner(\"Generating text...\"):\n",
        "            model, tokenizer = shared_model.load_text_generator()\n",
        "            output = shared_model.generate_text(prompt, model, tokenizer, max_length=max_len)\n",
        "            st.subheader(\"ðŸ“˜ Generated Text:\")\n",
        "            st.success(output)\n",
        "\n",
        "# --- Logging/Training Data (Optional) ---\n",
        "if not privacy_mode:\n",
        "    # Example: Save input/output to a local file for later training (disabled in privacy mode)\n",
        "    st.markdown(\"ðŸ“ Note: Logging enabled. Your inputs/outputs may be stored for training.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSCcd_QonY26",
        "outputId": "582604ab-534f-4e76-ea18-53f04cae54af"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-07 15:37:27.823 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.828 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.829 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.830 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.834 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.835 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.836 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.838 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.839 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.839 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.840 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.842 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.843 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.847 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.848 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.848 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.856 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.857 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.857 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.859 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.859 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.860 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.860 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.861 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.866 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.870 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.870 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.871 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.875 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.875 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.876 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-07 15:37:27.876 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2vPAruBIXPpRUDUe1f0G9Qd2weJ_5rp9nZkGPXhj9ZiPasao6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAsQiW2jqOe-",
        "outputId": "e0ae5d36-d4c0-4d58-bb12-4b82af036380"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/drive/MyDrive/TrustVault/TrustVault_Federated -type d -name \"__pycache__\" -exec rm -r {} +\n"
      ],
      "metadata": {
        "id": "Bc02LyWMqeW-"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# Clean up: kill previous streamlit/ngrok\n",
        "!pkill streamlit\n",
        "!pkill ngrok\n",
        "\n",
        "# Disconnect ngrok sessions\n",
        "try:\n",
        "    ngrok.kill()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(\"âœ… All previous ngrok & streamlit processes terminated.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_cFuFw1rRsc",
        "outputId": "8ca206ee-1d9a-4949-ac96-e1a54e2313eb"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-04-07T15:37:35+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-579291a1-b573-44ef-b241-ab980d1ca393 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-04-07T15:37:35+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-8501-579291a1-b573-44ef-b241-ab980d1ca393 err=\"failed to start tunnel: session closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All previous ngrok & streamlit processes terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "# Set your authtoken only ONCE per session\n",
        "ngrok.set_auth_token(\"2vPAruBIXPpRUDUe1f0G9Qd2weJ_5rp9nZkGPXhj9ZiPasao6\")  # Replace with your actual token\n",
        "\n",
        "# Start tunnel\n",
        "public_url = ngrok.connect(addr=8501, proto=\"http\")\n",
        "print(\"ðŸ”— Public Streamlit URL:\", public_url)\n",
        "\n",
        "# Wait a bit to let it settle\n",
        "time.sleep(2)\n",
        "\n",
        "# Start Streamlit\n",
        "!streamlit run /content/drive/MyDrive/TrustVault/TrustVault_Federated/trustvault_app.py &>/dev/null &\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcIFVwJ2qeTn",
        "outputId": "1e0667c0-4e83-4583-ecaf-0a85be516c7f"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”— Public Streamlit URL: NgrokTunnel: \"https://e2bc-34-91-248-18.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1u1SfwzqvqC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}